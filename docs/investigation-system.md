# Workflow Investigation System

## Overview

This homelab stack includes a comprehensive investigation system for analyzing n8n workflow executions, identifying issues, and providing actionable recommendations.

## Available Commands

### `/investigate <execution_id>`

**Purpose**: Comprehensive forensic analysis of a workflow execution

**What it does:**
- Gathers complete execution data and metrics
- Analyzes LLM performance and data quality
- Identifies root causes of issues
- Provides prioritized, actionable recommendations
- Generates detailed markdown investigation report
- Saves report to `docs/investigations/`

**When to use:**
- Workflow failed or behaved unexpectedly
- Performance degradation detected
- Data quality issues observed
- Need detailed analysis for optimization
- Want documented investigation for future reference

**Example:**
```bash
/investigate 194
```

**Output:**
- Terminal summary of findings
- Full markdown report in `docs/investigations/`
- Specific code examples and fixes
- Quantified impact estimates

---

### `/diagnose-workflow [execution_id]`

**Purpose**: Quick triage and immediate guidance

**What it does:**
- Fast analysis of latest or specified execution
- Identifies critical issues immediately
- Provides one-line status and quick fix
- Recommends escalation to full investigation if needed

**When to use:**
- Quick health check needed
- Want fast answer without full report
- Checking if investigation is warranted
- Rapid troubleshooting

**Example:**
```bash
/diagnose-workflow          # Analyze latest execution
/diagnose-workflow 194      # Quick check specific execution
```

**Output:**
- Concise terminal summary only
- Key metrics and immediate fix
- Link to run full investigation

---

## Investigation Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Workflow Execution Completes           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”œâ”€ Seems fine â†’ Done âœ…
                 â”‚
                 â”œâ”€ Quick question?
                 â”‚  â””â”€â†’ /diagnose-workflow
                 â”‚       â”œâ”€ Simple issue â†’ Apply quick fix âœ…
                 â”‚       â””â”€ Complex â†’ Escalate â†“
                 â”‚
                 â””â”€ Need detailed analysis?
                    â””â”€â†’ /investigate <id>
                         â”œâ”€ Generate report
                         â”œâ”€ Implement recommendations
                         â””â”€ Monitor next execution âœ…
```

## Investigation Reports

### Structure

All investigation reports follow this standard structure:

1. **Executive Summary**: Key findings and impact
2. **Execution Details**: Metrics, timing, status
3. **Analysis Sections**: Performance, data quality, model performance
4. **Root Cause Analysis**: Primary issues and contributing factors
5. **Recommendations**: Prioritized actions (immediate/short-term/long-term)
6. **Testing Recommendations**: Validation approach
7. **Conclusion**: Summary and action plan
8. **Appendix**: Technical details, commands used, metrics table

### Report Location

`docs/investigations/YYYY-MM-DD-workflow-[execution-id]-[brief-description].md`

### Existing Reports

- **2025-10-29-workflow-191-llm-parsing-failures.md**
  - Issue: 25% LLM JSON parsing failures
  - Root cause: Missing `format: json` parameter
  - Fix: Add format parameter to Ollama API call
  - Impact: 75% â†’ 100% success rate

- **2025-10-30-workflow-194-performance-analysis.md** (if created)
  - Issue: 46-minute processing time for 1 email
  - Root cause: Raw HTML input without preprocessing
  - Fix: Add HTML-to-text conversion
  - Impact: 46 min â†’ 5-10 min (estimated)

## Analysis Capabilities

The investigation system can analyze:

### Performance Metrics
- â±ï¸ Execution duration (total and per-node)
- ğŸ“Š Throughput (emails/minute, items processed)
- ğŸ”„ Comparison with historical executions
- ğŸ¯ Bottleneck identification
- ğŸ’¾ Resource utilization patterns

### Data Quality
- âœ… JSON parsing success rates
- ğŸ“ Schema compliance
- ğŸ” Content extraction quality
- ğŸŒ Multilingual handling
- âš ï¸ Error patterns and edge cases

### Model Performance
- ğŸ¤– Model identification and verification
- ğŸ’ª Capability assessment
- ğŸ“ Token usage and context limits
- ğŸ¨ Prompt effectiveness
- âš™ï¸ Configuration validation (format parameters, etc.)

### Workflow Structure
- ğŸ—ï¸ Node efficiency
- ğŸ” Loop optimization
- ğŸ›¡ï¸ Error handling quality
- ğŸ“¥ Input preprocessing
- ğŸ“¤ Output formatting

## Best Practices

### When to Investigate

**Always investigate:**
- âŒ Execution failures or errors
- ğŸŒ Significant performance degradation (>2x slower)
- ğŸ“‰ Data quality issues (>10% parsing failures)
- ğŸ”„ Repeating problems (same issue 2+ times)

**Consider investigating:**
- ğŸ¤” Unexpected behavior (works but output is weird)
- ğŸ² Inconsistent results (sometimes works, sometimes doesn't)
- ğŸ” Before optimization (establish baseline)

**Quick diagnose sufficient:**
- âœ… Success but want to verify
- âš¡ Quick performance check
- ğŸ“Š Routine health check

### Investigation Tips

1. **Verify the model**: Always ask user if model was changed via UI
2. **Check recent history**: Context from `exec-history 10` is valuable
3. **Compare wisely**: Don't auto-compare unless meaningful baseline exists
4. **Quantify impact**: Use numbers (46 min â†’ 5 min, 75% â†’ 100%)
5. **Provide examples**: Show actual code, not just descriptions
6. **Prioritize clearly**: Immediate vs. short-term vs. long-term

### After Investigation

1. **Review the report** - Check findings make sense
2. **Implement high-priority fixes** - Start with immediate actions
3. **Test changes** - Run new execution to validate
4. **Monitor next executions** - Verify improvement
5. **Update documentation** - Capture lessons learned

## Manual Investigation Commands

If you prefer manual investigation without the slash commands:

```bash
# Get execution details
./scripts/manage.sh exec-details <execution_id>

# Analyze LLM responses
./scripts/manage.sh exec-llm <execution_id>

# Parse all node outputs
./scripts/manage.sh exec-parse <execution_id>

# Extract raw data
./scripts/manage.sh exec-data <execution_id> output.json

# Check recent history
./scripts/manage.sh exec-history 10

# View failed executions
./scripts/manage.sh exec-failed 10

# Get statistics
./scripts/manage.sh exec-stats
```

See `CLAUDE.md` for complete command reference.

## Integration with Workflow Development

### Development Cycle

```
1. Develop workflow in n8n UI
   â†“
2. Test with sample data
   â†“
3. Run /diagnose-workflow â†’ Quick check
   â†“
4. If issues: /investigate <id> â†’ Get report
   â†“
5. Implement recommendations
   â†“
6. Test again
   â†“
7. Export workflow: ./scripts/manage.sh export-workflows
   â†“
8. Commit changes with investigation report
```

### Before Production

Run comprehensive investigation on:
- âœ… Successful test execution (establish baseline)
- âŒ Edge case/failure execution (verify error handling)
- ğŸ”„ Batch execution (verify scalability)

Document baseline metrics for future comparison.

## Troubleshooting the Investigation System

### Investigation command not found

```bash
# Verify command exists
ls -la .claude/commands/investigate.md

# If missing, it should be created automatically
# Check Claude Code settings
```

### Can't access execution data

```bash
# Verify PostgreSQL is running
./scripts/manage.sh status

# Check if execution ID exists
./scripts/manage.sh exec-history 20
```

### Report not generated

Check:
1. Do you have write permissions to `docs/investigations/`?
2. Is there disk space available?
3. Did the investigation complete without errors?

## Future Enhancements

Potential additions to the investigation system:

- ğŸ”” **Automatic alerting**: Monitor executions and auto-investigate failures
- ğŸ“ˆ **Trend analysis**: Track metrics over time, identify degradation patterns
- ğŸ”¬ **A/B comparison**: Compare two executions side-by-side
- ğŸ¯ **Recommendation tracking**: Mark which fixes were implemented
- ğŸ“Š **Dashboard**: Visual overview of workflow health
- ğŸ¤– **Auto-fix**: Implement simple fixes automatically (with approval)

---

## Examples

### Example 1: LLM Parsing Failure Investigation

```
/investigate 191

â†’ Identified: 25% JSON parsing failures (5/20 emails)
â†’ Root cause: Missing `format: json` parameter
â†’ Fix: Add parameter to Ollama API call
â†’ Report: docs/investigations/2025-10-29-workflow-191-llm-parsing-failures.md
â†’ Result: Next execution 100% success rate âœ…
```

### Example 2: Performance Issue Diagnostic

```
/diagnose-workflow 194

â†’ Quick finding: 46 min for 1 email (4x slower than expected)
â†’ Immediate cause: Sending 127KB raw HTML to LLM
â†’ Quick fix: Add HTML preprocessing
â†’ Escalated: /investigate 194 for detailed analysis
```

### Example 3: Routine Health Check

```
/diagnose-workflow

â†’ Latest execution: 195 | 5.2 min | Success âœ…
â†’ LLM: 100% JSON parsing | Model: llama3.1:8b
â†’ Performance: Within expected range
â†’ Status: All systems operational
```

---

**Last Updated**: 2025-10-30
**Maintained By**: Workflow Investigation Agent

# ğŸ  Homelab Automation Stack

Self-hosted automation platform for Raspberry Pi 5/macOS. Automate workflows with n8n, Telegram, Notion, Gmail, and local LLMs.

## âœ¨ Features

- ğŸ“± **Telegram â†’ Notion**: Convert messages to tasks with AI processing
- ğŸ“§ **Gmail Summaries**: Daily email digests via Telegram
- ğŸ¤– **Local LLMs**: Privacy-first AI processing with Ollama
- ğŸ”„ **Containerized**: Fully dockerized with automated backups
- ğŸŒ **Secure Access**: Optional Tailscale integration

## ğŸ› ï¸ Stack

- **n8n** - Workflow automation platform
- **PostgreSQL** - Reliable database backend
- **Ollama** - Local LLM inference
- **Tailscale** - Zero-trust network access (optional)

## ğŸš€ Quick Start

```bash
# 1. Clone repository
git clone <your-repo> homelab-stack && cd homelab-stack

# 2. Setup everything
./scripts/setup.sh

# 3. Configure APIs
cp .env.example .env
nano .env  # Add your tokens
```

**Required**: `TELEGRAM_BOT_TOKEN` from [@BotFather](https://t.me/BotFather)  
**Optional**: Notion and Gmail API tokens

**Access**: n8n at `https://localhost:8443`, Ollama at `http://localhost:11434`

## ğŸ“‹ Management

```bash
./scripts/manage.sh status       # Check services
./scripts/manage.sh logs         # View logs
./scripts/manage.sh exec-latest  # Show workflow executions
./scripts/backup.sh             # Create backup
./scripts/setup.sh funnel       # Enable external webhooks
```

Run any script without arguments to see all options.

## ğŸ”§ Hardware Recommendations

**This project uses a Raspberry Pi 5 with 16GB RAM** - ideal for running larger models like qwen2.5:14b.

| Device            | RAM  | Models        | Performance      |
| ----------------- | ---- | ------------- | ---------------- |
| **Pi 5 4GB**      | 4GB  | `llama3.2:1b` | Basic automation |
| **Pi 5 8GB**      | 8GB  | `llama3.1:8b` | Good performance |
| **Pi 5 16GB** â­   | 16GB | `qwen2.5:14b` to `qwen2.5:32b` | High performance (current setup) |
| **Apple Silicon** | 8GB+ | Any models    | Excellent        |

## ğŸ“š Documentation

- [ğŸš€ **Setup Guide**](docs/setup-guide.md) - Complete installation walkthrough
- [ğŸŒ **Tailscale Setup**](docs/tailscale-setup.md) - SSL certificates and external access
- [ğŸ”— **API Configuration**](docs/api-setup.md) - Telegram, Notion, Gmail setup
- [âš¡ **Workflow Management**](docs/workflows.md) - Import/export and examples
- [ğŸ–¥ï¸ **Hardware Optimization**](docs/hardware-setup.md) - Platform-specific tuning

## ï¿½ Long-Running LLM Calls

This stack includes a timeout patch to enable AI Agent nodes with long-running LLM calls (>5 minutes). The default Node.js and undici timeouts are too restrictive for complex AI processing.

**Solution Applied**: [n8n-timeout-patch](https://github.com/Piggeldi2013/n8n-timeout-patch) by [@Piggeldi2013](https://github.com/Piggeldi2013)

**How it works**:

- Patches Node.js HTTP server timeouts (inbound requests)
- Configures undici global dispatcher (outbound API calls to LLMs)
- Preloaded via `NODE_OPTIONS=--require /patch/patch-http-timeouts.js`

**Timeout Configuration**:

- **Workflow execution**: 6 hours (`N8N_WORKFLOW_TIMEOUT=21600`)
- **LLM headers response**: 30 minutes (`FETCH_HEADERS_TIMEOUT=1800000`)
- **LLM body streaming**: 3.33 hours (`FETCH_BODY_TIMEOUT=12000000`)

**Files**:

- `config/n8n/patch-http-timeouts.js` - Timeout patch script
- Environment variables in `docker-compose.yml` - Timeout configuration

**Verification**: Check logs for `[patch] undici dispatcher set` and `[patch] http server timeouts` messages.

## ï¿½ğŸš¨ Troubleshooting

```bash
./scripts/manage.sh diagnose    # Full system check
./scripts/manage.sh restart     # Restart services
docker compose logs -f n8n      # View n8n logs
```

**Common Issues**: Port conflicts, SSL certificates, webhook setup â†’ See setup guide

## ğŸ“„ License

MIT License - For personal use. Review security before internet exposure.
